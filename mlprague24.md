https://shorturl.at/hITW4

bit.ly/3Q7X5Ft 

https://drive.google.com/drive/u/0/folders/1iUmI-6pmfMJaOy-jrz36dIi9wiwzTA_x 


bit.ly/praguellm   https://github.com/monitora-media/mlprague2024
platform,openai.com/tokenizer (shows us the tokenization)

openai tokens different number of tokens for en / cz (was trained on en ofc)

https://labs.perplexity.ai/

lmsys chatarenas


https://github.com/fkabs/mlprague_2024 


-----------------
murray reasoning.
"LLMs are not good for reasoning", maybe with interpreters added

seznam
contrastive learning
triplet loss. attract positive pairs, repel negative, repel all inone batch.
plus use clicks from users with care as they click on clickbaits
used sophisticated heuristic for finding good negative examples

quantum computing in ML
wht we can learn? Not everything is learnable. Quantum computer has a big overhead. small problems will be more efficiant on classical computers.
there are even too hard problems. you can verify things by qc, but there are uncomputable things. we can just do it more efficiently.
everything with QC and ML QC we can do classically (if we have infinite time and space) , but not 
therefore "quantum efficient" problems, and for quantum hard we can at least verify.
exponentially faster or exponentially fewer samples (quantum PCA).
and model capacity (linear model not enough).
quantum tunneling over loss landscape. but there might be some difficulties (decay etc)
adiabatic
variational
non variational
quantum inspired algorithms (tensor networks), hedge funds interested for speedups

quantum comp can help:
natural sciences, quantum technologies, enhance classical ML
fundamental research underlies everything

quantum in natural sciences
learning quantum states - tells chemistry companies how muc henergy and reaction chain, ground and excited state is a business problem
learn observables (hamiltonian)
learning quantum processes
quantum learning to enhance ml - still not better, lots of challenges. we have algorithms but we wait for the devices to run efficiently, certain things we cannot foresee in the tech and algo progress

expressability vs learnability.
flat landscape happens very quickly.

dequantizing - you look at algorithm and find later that it can be more efficient on a classical device.

there are cases where we can learn quantum states with exponencially few data (you use only 1 sample where classical M needs hundreds...)



------------------------------
perspective taking - vital to empathy.
models do not handle it consistently.

how to measure it? psychology testing false beliefs (closed box, one of them sees that there is a popcorn in the box, you close it and write chocolate and you ask one what the other is thinking what is inside the box .... and they can answer correctly from 6 years old, so able to take perspective.)

how to model perspective taking? What is the next natural thing after you break a glass etc, you collect large databases of that.
also what are the individual specifics, some things same, lots of things addressed differently (world knowledge, personal knowledge, social knowledge)
perspective taking needs more than one right answer
am i the asshole reddit as sources. did i do something wrong? enough comments mean we can model even the commenters
person grounding - judgement generation fro mituation and person description
(i am ai think i want)
persona embeddings, dataset from what a user ever said and style...
personna grounded transformers. attention over user and attention over ituation. or decoder
evaluation - human evaluators said mostly yes, they needed ab testing

results: generated same human was better than incorrect human